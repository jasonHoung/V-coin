# 開發實作指南

## Phase 1: MVP 推論服務（0-3個月）

### 目標
建立基礎推論服務，支援 LLM 模型，完成 GPU Agent 與平台核心功能。

### 1.1 平台後端開發

#### 技術棧
- 語言: Go 或 Rust
- 資料庫: PostgreSQL + Redis
- API: RESTful + WebSocket

#### 核心模組

**任務管理 API**
```go
// POST /api/v1/inference
type InferenceRequest struct {
    ModelID    string                 `json:"model_id"`
    Input      string                 `json:"input"`
    Parameters map[string]interface{} `json:"parameters"`
    Priority   int                    `json:"priority"`
}

// GET /api/v1/task/:id/status
type TaskStatus struct {
    TaskID     string `json:"task_id"`
    Status     string `json:"status"` // pending, running, completed, failed
    Progress   int    `json:"progress"`
    Result     string `json:"result,omitempty"`
    NodeID     string `json:"node_id,omitempty"`
}
```

**節點管理 API**
```go
// POST /api/v1/node/heartbeat
type HeartbeatRequest struct {
    NodeID         string  `json:"node_id"`
    GPUModel       string  `json:"gpu_model"`
    GPUMemory      int     `json:"gpu_memory"`
    GPUUtilization float64 `json:"gpu_utilization"`
    Temperature    float64 `json:"temperature"`
    Status         string  `json:"status"`
}

// GET /api/v1/node/:id/task
type TaskAssignment struct {
    TaskID     string                 `json:"task_id"`
    Type       string                 `json:"type"`
    ModelID    string                 `json:"model_id"`
    ModelURL   string                 `json:"model_url"`
    Input      string                 `json:"input"`
    Parameters map[string]interface{} `json:"parameters"`
    Timeout    int                    `json:"timeout"`
}
```

#### 資料庫設計
```sql
-- 任務表
CREATE TABLE tasks (
    id UUID PRIMARY KEY,
    user_id UUID NOT NULL,
    type VARCHAR(50) NOT NULL,
    model_id VARCHAR(100) NOT NULL,
    input TEXT NOT NULL,
    parameters JSONB,
    status VARCHAR(20) NOT NULL,
    result TEXT,
    node_id UUID,
    created_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    retry_count INT DEFAULT 0
);

-- 節點表
CREATE TABLE nodes (
    id UUID PRIMARY KEY,
    gpu_model VARCHAR(100) NOT NULL,
    gpu_memory INT NOT NULL,
    cuda_version VARCHAR(20),
    reputation_score FLOAT DEFAULT 100,
    staked_amount BIGINT DEFAULT 0,
    status VARCHAR(20) NOT NULL,
    last_heartbeat TIMESTAMP,
    created_at TIMESTAMP NOT NULL
);

-- 驗證記錄表
CREATE TABLE validations (
    id UUID PRIMARY KEY,
    task_id UUID REFERENCES tasks(id),
    validator_node_id UUID REFERENCES nodes(id),
    original_result TEXT,
    validation_result TEXT,
    similarity_score FLOAT,
    passed BOOLEAN,
    created_at TIMESTAMP NOT NULL
);
```

### 1.2 GPU Agent 開發

#### Agent 架構
```python
# agent/main.py
class GPUAgent:
    def __init__(self, config):
        self.node_id = config.node_id
        self.platform_url = config.platform_url
        self.gpu_id = config.gpu_id
        self.heartbeat_interval = 30
        
    def start(self):
        # 啟動心跳線程
        threading.Thread(target=self.heartbeat_loop).start()
        
        # 啟動任務接收線程
        threading.Thread(target=self.task_loop).start()
        
    def heartbeat_loop(self):
        while True:
            status = self.collect_gpu_status()
            response = self.send_heartbeat(status)
            
            if response.get('has_task'):
                self.fetch_and_execute_task()
            
            time.sleep(self.heartbeat_interval)
    
    def collect_gpu_status(self):
        # 使用 nvidia-smi 收集資訊
        import pynvml
        pynvml.nvmlInit()
        handle = pynvml.nvmlDeviceGetHandleByIndex(self.gpu_id)
        
        return {
            'gpu_model': pynvml.nvmlDeviceGetName(handle),
            'gpu_memory': pynvml.nvmlDeviceGetMemoryInfo(handle).total,
            'gpu_utilization': pynvml.nvmlDeviceGetUtilizationRates(handle).gpu,
            'temperature': pynvml.nvmlDeviceGetTemperature(handle, 0)
        }
```

#### Docker 執行器
```python
# agent/executor.py
class DockerExecutor:
    def __init__(self):
        self.client = docker.from_env()
        
    def run_inference(self, task):
        # 準備掛載目錄
        model_dir = self.prepare_model(task.model_id)
        input_file = self.write_input(task.input)
        output_dir = tempfile.mkdtemp()
        
        # 啟動容器
        container = self.client.containers.run(
            image='vcoin/inference:latest',
            runtime='nvidia',
            environment={
                'NVIDIA_VISIBLE_DEVICES': str(self.gpu_id),
                'MODEL_PATH': '/models',
                'INPUT_FILE': '/input/data.json',
                'OUTPUT_DIR': '/output'
            },
            volumes={
                model_dir: {'bind': '/models', 'mode': 'ro'},
                input_file: {'bind': '/input/data.json', 'mode': 'ro'},
                output_dir: {'bind': '/output', 'mode': 'rw'}
            },
            detach=True,
            remove=True
        )
        
        # 等待完成
        result = container.wait(timeout=task.timeout)
        
        # 讀取結果
        output = self.read_output(output_dir)
        
        return output
```

### 1.3 推論容器開發

#### Dockerfile
```dockerfile
# docker/inference/Dockerfile
FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

# 安裝 Python 與依賴
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# 安裝 PyTorch 與相關庫
RUN pip3 install --no-cache-dir \
    torch==2.0.0 \
    transformers==4.30.0 \
    accelerate==0.20.0

# 複製執行腳本
COPY inference.py /app/
WORKDIR /app

# 建立非 root 使用者
RUN useradd -m -u 1000 vcoin
USER vcoin

CMD ["python3", "inference.py"]
```

#### 推論腳本
```python
# docker/inference/inference.py
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def main():
    # 讀取配置
    model_path = os.environ['MODEL_PATH']
    input_file = os.environ['INPUT_FILE']
    output_dir = os.environ['OUTPUT_DIR']
    
    # 載入模型
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map='auto'
    )
    
    # 讀取輸入
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    # 執行推論
    inputs = tokenizer(data['text'], return_tensors='pt').to('cuda')
    outputs = model.generate(
        **inputs,
        max_length=data.get('max_length', 100),
        temperature=data.get('temperature', 0.8)
    )
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # 寫入結果
    with open(f'{output_dir}/result.json', 'w') as f:
        json.dump({'output': result}, f)

if __name__ == '__main__':
    main()
```

### 1.4 部署與測試

#### 本地開發環境
```bash
# 啟動平台後端
cd platform
go run main.go

# 啟動 Redis 與 PostgreSQL
docker-compose up -d redis postgres

# 啟動 GPU Agent
cd agent
python main.py --config config.yaml
```

#### 測試流程
```bash
# 1. 註冊節點
curl -X POST http://localhost:8080/api/v1/node/register \
  -d '{"gpu_model": "RTX 3090", "gpu_memory": 24576}'

# 2. 提交推論任務
curl -X POST http://localhost:8080/api/v1/inference \
  -d '{
    "model_id": "llama-7b",
    "input": "介紹人工智慧",
    "parameters": {"max_length": 100}
  }'

# 3. 查詢結果
curl http://localhost:8080/api/v1/task/{task_id}/status
```

---

## Phase 2: LoRA 訓練與 Coin 上線（3-6個月）

### 目標
實現 LoRA 分散式訓練、Coin 智能合約、質押與獎勵系統。

### 2.1 LoRA 訓練實作

#### 訓練任務分配
```python
# platform/training_scheduler.py
class TrainingScheduler:
    def split_dataset(self, dataset_path, num_shards):
        """將資料集分片"""
        dataset = load_dataset(dataset_path)
        shard_size = len(dataset) // num_shards
        
        shards = []
        for i in range(num_shards):
            start = i * shard_size
            end = start + shard_size if i < num_shards - 1 else len(dataset)
            shards.append(dataset[start:end])
        
        return shards
    
    def assign_training_tasks(self, user_request):
        # 選擇可用節點
        nodes = select_training_nodes(
            gpu_requirement=user_request.gpu_type,
            count=user_request.num_gpus
        )
        
        # 分片資料
        shards = self.split_dataset(
            user_request.dataset, 
            len(nodes)
        )
        
        # 建立任務
        tasks = []
        for node, shard in zip(nodes, shards):
            task = {
                'node_id': node.id,
                'type': 'lora_training',
                'base_model': user_request.model,
                'dataset_shard': upload_shard(shard),
                'lora_config': user_request.lora_config,
                'epochs': user_request.epochs
            }
            tasks.append(task)
        
        return tasks
```

#### LoRA 訓練容器
```python
# docker/training/train_lora.py
from peft import LoraConfig, get_peft_model
from transformers import Trainer, TrainingArguments

def train_lora(config):
    # 載入基礎模型
    model = AutoModelForCausalLM.from_pretrained(config.base_model)
    
    # 配置 LoRA
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05
    )
    
    model = get_peft_model(model, lora_config)
    
    # 載入資料
    dataset = load_from_disk(config.dataset_path)
    
    # 訓練參數
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.epochs,
        per_device_train_batch_size=config.batch_size,
        gradient_accumulation_steps=4,
        save_steps=config.gradient_sync_interval,
        logging_steps=10
    )
    
    # 自定義 Callback：定期上傳梯度
    class GradientSyncCallback(TrainerCallback):
        def on_save(self, args, state, control, **kwargs):
            # 上傳 LoRA 權重至平台
            upload_weights(kwargs['model'], state.global_step)
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        callbacks=[GradientSyncCallback()]
    )
    
    trainer.train()
    
    # 上傳最終權重
    trainer.save_model(config.output_dir)
    upload_final_weights(config.output_dir)
```

### 2.2 Coin 智能合約

#### Solidity 合約（使用 EVM 鏈）
```solidity
// contracts/VCoin.sol
pragma solidity ^0.8.0;

import "@openzeppelin/contracts/token/ERC20/ERC20.sol";
import "@openzeppelin/contracts/access/AccessControl.sol";

contract VCoin is ERC20, AccessControl {
    bytes32 public constant MINTER_ROLE = keccak256("MINTER_ROLE");
    bytes32 public constant BURNER_ROLE = keccak256("BURNER_ROLE");
    
    uint256 public constant MAX_SUPPLY = 1_000_000_000 * 10**18;
    
    mapping(address => uint256) public stakedAmount;
    
    event Staked(address indexed node, uint256 amount);
    event Unstaked(address indexed node, uint256 amount);
    event Slashed(address indexed node, uint256 amount);
    
    constructor() ERC20("VCoin", "VCOIN") {
        _setupRole(DEFAULT_ADMIN_ROLE, msg.sender);
        _setupRole(MINTER_ROLE, msg.sender);
    }
    
    function mint(address to, uint256 amount) external onlyRole(MINTER_ROLE) {
        require(totalSupply() + amount <= MAX_SUPPLY, "Exceeds max supply");
        _mint(to, amount);
    }
    
    function burn(uint256 amount) external {
        _burn(msg.sender, amount);
    }
    
    function stake(uint256 amount) external {
        require(balanceOf(msg.sender) >= amount, "Insufficient balance");
        transfer(address(this), amount);
        stakedAmount[msg.sender] += amount;
        emit Staked(msg.sender, amount);
    }
    
    function unstake(uint256 amount) external {
        require(stakedAmount[msg.sender] >= amount, "Insufficient staked");
        stakedAmount[msg.sender] -= amount;
        _transfer(address(this), msg.sender, amount);
        emit Unstaked(msg.sender, amount);
    }
    
    function slash(address node, uint256 amount) external onlyRole(DEFAULT_ADMIN_ROLE) {
        require(stakedAmount[node] >= amount, "Insufficient staked");
        stakedAmount[node] -= amount;
        
        // 50% 燃燒，50% 進入儲備
        uint256 burnAmount = amount / 2;
        uint256 reserveAmount = amount - burnAmount;
        
        _burn(address(this), burnAmount);
        _transfer(address(this), msg.sender, reserveAmount);
        
        emit Slashed(node, amount);
    }
}
```

#### 部署腳本
```javascript
// scripts/deploy.js
const { ethers } = require("hardhat");

async function main() {
    const VCoin = await ethers.getContractFactory("VCoin");
    const vcoin = await VCoin.deploy();
    await vcoin.deployed();
    
    console.log("VCoin deployed to:", vcoin.address);
    
    // 設定平台為 Minter
    const MINTER_ROLE = await vcoin.MINTER_ROLE();
    await vcoin.grantRole(MINTER_ROLE, process.env.PLATFORM_ADDRESS);
}

main().catch((error) => {
    console.error(error);
    process.exit(1);
});
```

### 2.3 獎勵結算系統

```python
# platform/reward_manager.py
class RewardManager:
    def calculate_and_distribute(self, task):
        """計算並發放獎勵"""
        
        # 計算獎勵
        if task.type == 'inference':
            reward = self.calculate_inference_reward(task)
        elif task.type == 'training':
            reward = self.calculate_training_reward(task)
        
        # 發放到鏈上
        tx = self.mint_vcoin(
            to=task.node_wallet_address,
            amount=reward
        )
        
        # 記錄
        self.log_reward(task.id, task.node_id, reward, tx.hash)
    
    def mint_vcoin(self, to, amount):
        """呼叫智能合約鑄造 Coin"""
        web3 = Web3(Web3.HTTPProvider(self.rpc_url))
        contract = web3.eth.contract(
            address=self.vcoin_address,
            abi=self.vcoin_abi
        )
        
        tx = contract.functions.mint(
            to, 
            int(amount * 10**18)
        ).build_transaction({
            'from': self.platform_address,
            'nonce': web3.eth.get_transaction_count(self.platform_address)
        })
        
        signed_tx = web3.eth.account.sign_transaction(tx, self.private_key)
        tx_hash = web3.eth.send_raw_transaction(signed_tx.rawTransaction)
        
        return web3.eth.wait_for_transaction_receipt(tx_hash)
```

---

## Phase 3: 商用化與治理（6-12個月）

### 目標
企業級功能、SLA 保證、DAO 治理框架。

### 3.1 SLA 服務

```python
# platform/sla_manager.py
class SLAManager:
    def create_sla_contract(self, customer, terms):
        """建立 SLA 合約"""
        sla = {
            'customer_id': customer.id,
            'guaranteed_uptime': terms.uptime,  # 99.9%
            'max_latency': terms.max_latency,   # 5s
            'min_throughput': terms.throughput, # 1000 req/day
            'price': terms.price,
            'penalty_rate': 0.1  # 違約賠償 10%
        }
        
        return self.db.create_sla(sla)
    
    def monitor_sla_compliance(self, sla_id):
        """監控 SLA 履約狀況"""
        sla = self.db.get_sla(sla_id)
        metrics = self.get_metrics(sla.customer_id)
        
        violations = []
        
        if metrics.uptime < sla.guaranteed_uptime:
            violations.append('uptime')
        
        if metrics.avg_latency > sla.max_latency:
            violations.append('latency')
        
        if violations:
            self.handle_sla_violation(sla, violations)
```

### 3.2 治理機制

```solidity
// contracts/Governance.sol
contract VCoinGovernance {
    struct Proposal {
        uint256 id;
        string description;
        uint256 forVotes;
        uint256 againstVotes;
        uint256 endTime;
        bool executed;
    }
    
    mapping(uint256 => Proposal) public proposals;
    uint256 public proposalCount;
    
    function createProposal(string memory description) external {
        require(vcoin.balanceOf(msg.sender) >= 1000 * 10**18, "Need 1000 VCOIN to propose");
        
        proposals[proposalCount] = Proposal({
            id: proposalCount,
            description: description,
            forVotes: 0,
            againstVotes: 0,
            endTime: block.timestamp + 7 days,
            executed: false
        });
        
        proposalCount++;
    }
    
    function vote(uint256 proposalId, bool support) external {
        uint256 votingPower = vcoin.balanceOf(msg.sender);
        require(votingPower > 0, "No voting power");
        
        if (support) {
            proposals[proposalId].forVotes += votingPower;
        } else {
            proposals[proposalId].againstVotes += votingPower;
        }
    }
}
```

---

## 附錄：開發資源

### A. 依賴項目
- **GPU Agent**: Python 3.10+, PyTorch, Docker SDK
- **Platform**: Go 1.20+ / Rust 1.70+
- **Database**: PostgreSQL 14+, Redis 7+
- **Blockchain**: Hardhat, Web3.py

### B. 測試環境
- 本地：單 GPU 模擬
- Testnet：部署至 Polygon Mumbai / Goerli
- 驗證：使用模擬節點測試分散式流程

### C. 參考文件
- [Hugging Face PEFT 文檔](https://huggingface.co/docs/peft)
- [NVIDIA Docker 指南](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/)
- [OpenZeppelin 合約](https://docs.openzeppelin.com/contracts/)
